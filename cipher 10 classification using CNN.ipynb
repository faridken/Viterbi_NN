{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp; import utils; imp.reload(utils); from utils import *; import numpy as np\n",
    "import pandas as pd; import math; import time; from copy import deepcopy;\n",
    "import matplotlib.pyplot as plt; import warnings; from matplotlib import rc;\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rc('text', usetex = True)\n",
    "#rc('font', **font)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "from matplotlib import rc\n",
    "font = {'family' : 'serif',}\n",
    "rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d56538e77e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestIndex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "result = model.predict(test_images)\n",
    "for i in testIndex:\n",
    "    print(np.argmax(result[i]), test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testIndex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7626eb9f5448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testIndex' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from random import randrange\n",
    "# testIndex =[]\n",
    "# for i in range(20):\n",
    "#     testIndex.append(randrange(10000))\n",
    "    \n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,15))\n",
    "for i, val in enumerate(testIndex):\n",
    "    plt.subplot(5,4,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(test_images[val])\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "#     plt.xlabel(class_names[test_labels[val][0]])\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=2, wspace=1, hspace=7)\n",
    "    plt.tight_layout()\n",
    "plt.text(-150,-5, \"Python:\")\n",
    "plt.text(-150,-15, \"SMART:\")\n",
    "\n",
    "\n",
    "plt.text(-150,-65, \"Python:\")\n",
    "plt.text(-150,-75, \"SMART:\")\n",
    "\n",
    "# plt.text(-150,-35, \"Python:\")\n",
    "# plt.text(-150,-45, \"SMART:\")\n",
    "\n",
    "plt.text(-150,-125, \"Python:\")\n",
    "plt.text(-150,-135, \"SMART:\")\n",
    "\n",
    "\n",
    "plt.text(-150,-185, \"Python:\")\n",
    "plt.text(-150,-195, \"SMART:\")\n",
    "\n",
    "\n",
    "plt.text(-150,55, \"Python:\")\n",
    "plt.text(-150,45, \"SMART:\")\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        plt.text(-110 + i * 38.5, -185 + j * 60, \"%s\"%(class_names[int(np.argmax(result[testIndex[i + j * 4]]))]))\n",
    "        plt.text(-110 + i * 38.5, -195 + j * 60, \"%s\"%(class_names[int(np.argmax(result[testIndex[i + j * 4]]))]))\n",
    "    \n",
    "\n",
    "plt.savefig('cifar.png', dpi = 300) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_MACs_in_Conv():\n",
    "    K = 3\n",
    "    C_in = 3\n",
    "    C_out = 32\n",
    "    H_out = 30\n",
    "    W_out = 30   \n",
    "    return (K**2) * C_in * H_out * W_out * C_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_MACs_in_Conv(K, C_in, C_out, H_out, W_out): \n",
    "    return (K**2) * C_in * H_out * W_out * C_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first conv\n",
    "conv1 = calc_MACs_in_Conv(3, 3, 32, 30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second conv\n",
    "conv2 = calc_MACs_in_Conv(3, 32, 32, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_MACs_in_FC(in_n, out_n): \n",
    "    return in_n * out_n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first fc\n",
    "fc1 = calc_MACs_in_FC(6272, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second fc\n",
    "fc2 = calc_MACs_in_FC(128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 + conv2 + fc1 + fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy %71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), use_bias=False,padding='valid', activation='relu', input_shape=(32,32,3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3,3), use_bias=False,padding='valid', activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, use_bias=False,activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, use_bias=False,activation='softmax'))    # num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_1 (QuantizeLa (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_2 (QuantizeWrap (None, 30, 30, 32)        931       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_3 (QuantizeWrap (None, 28, 28, 32)        9283      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_1 (Quant (None, 14, 14, 32)        1         \n",
      "_________________________________________________________________\n",
      "quant_dropout_2 (QuantizeWra (None, 14, 14, 32)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_1 (QuantizeWra (None, 6272)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_2 (QuantizeWrapp (None, 128)               802821    \n",
      "_________________________________________________________________\n",
      "quant_dropout_3 (QuantizeWra (None, 128)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_3 (QuantizeWrapp (None, 10)                1285      \n",
      "=================================================================\n",
      "Total params: 814,327\n",
      "Trainable params: 814,176\n",
      "Non-trainable params: 151\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 47s 119ms/step - loss: 2.1311 - accuracy: 0.3247 - val_loss: 2.0067 - val_accuracy: 0.4588\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 2.0179 - accuracy: 0.4429 - val_loss: 1.9535 - val_accuracy: 0.5084\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.9670 - accuracy: 0.4943 - val_loss: 1.9087 - val_accuracy: 0.5548\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 44s 111ms/step - loss: 1.9388 - accuracy: 0.5218 - val_loss: 1.8953 - val_accuracy: 0.5654\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 1.9164 - accuracy: 0.5443 - val_loss: 1.8782 - val_accuracy: 0.5835\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 1.8971 - accuracy: 0.5636 - val_loss: 1.8706 - val_accuracy: 0.5897\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 1.8853 - accuracy: 0.5745 - val_loss: 1.8510 - val_accuracy: 0.6092\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 1.8724 - accuracy: 0.5887 - val_loss: 1.8398 - val_accuracy: 0.6183\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 1.8600 - accuracy: 0.6013 - val_loss: 1.8404 - val_accuracy: 0.6197\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 1.8497 - accuracy: 0.6113 - val_loss: 1.8374 - val_accuracy: 0.6212\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 1.8431 - accuracy: 0.6174 - val_loss: 1.8223 - val_accuracy: 0.6390\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 1.8358 - accuracy: 0.6249 - val_loss: 1.8174 - val_accuracy: 0.6430\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 40s 101ms/step - loss: 1.8286 - accuracy: 0.6321 - val_loss: 1.8158 - val_accuracy: 0.6439\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 1.8247 - accuracy: 0.6355 - val_loss: 1.8215 - val_accuracy: 0.6372\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 1.8192 - accuracy: 0.6420 - val_loss: 1.8031 - val_accuracy: 0.6563\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 1.8120 - accuracy: 0.6484 - val_loss: 1.8071 - val_accuracy: 0.6540\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 1.8092 - accuracy: 0.6521 - val_loss: 1.8144 - val_accuracy: 0.6436\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 1.8063 - accuracy: 0.6545 - val_loss: 1.7976 - val_accuracy: 0.6623\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 1.7999 - accuracy: 0.6603 - val_loss: 1.7903 - val_accuracy: 0.6719\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 1.7972 - accuracy: 0.6640 - val_loss: 1.7958 - val_accuracy: 0.6627\n"
     ]
    }
   ],
   "source": [
    "history = q_aware_model.fit(train_images, train_labels, epochs=20, batch_size=128,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quant test accuracy: [1.7958474159240723, 0.6626999974250793]\n"
     ]
    }
   ],
   "source": [
    "# baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "q_aware_model_accuracy = q_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "# print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/farid/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/farid/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp9rsvqc37/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.1\n",
      "Quant TF test accuracy: [1.7958474159240723, 0.6626999974250793]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(model.summary())\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_images, train_labels, epochs=20, batch_size=128,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from random import randrange\n",
    "# testIndex =[]\n",
    "# for i in range(20):\n",
    "#     testIndex.append(randrange(10000))\n",
    "result = model.predict(test_images)\n",
    "acc = 20\n",
    "for i in testIndex:\n",
    "    print(np.argmax(result[i]), test_labels[i][0])\n",
    "    if np.argmax(result[i]) != test_labels[i][0]:\n",
    "        acc -= 1\n",
    "        \n",
    "print(acc / 20 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "model.summary()\n",
    "\n",
    "pred_digital = []\n",
    "pred_analog  = []\n",
    "ad = ['analog', 'digital']\n",
    "\n",
    "for sim in ad:\n",
    "    for i in range(5, 10):\n",
    "        inputs = train_images[i]\n",
    "        \n",
    "        (ix, iy, iz) = inputs.shape\n",
    "        print(\"input size\", inputs.shape)\n",
    "        # ------------------------------------------------------------------\n",
    "        if sim == 'analog':\n",
    "            ans = conv2dAnalog(inputs, model.layers[0].get_weights()[0])\n",
    "        elif sim == 'digital':\n",
    "            ans = conv2d(inputs, model.layers[0].get_weights()[0])\n",
    "            \n",
    "        ans = tf.keras.activations.relu(ans)\n",
    "        ans = ans.numpy()\n",
    "        # ------------------------------------------------------------------\n",
    "        model_batch = Sequential()\n",
    "        model_batch.add(layers.BatchNormalization())\n",
    "        ans = model_batch.predict(ans)        \n",
    "        # ------------------------------------------------------------------\n",
    "#         if sim == 'analog':\n",
    "#             ans = conv2dAnalog(ans, model.layers[2].get_weights()[0])\n",
    "#         elif sim == 'digital':\n",
    "#             ans = conv2d(ans, model.layers[2].get_weights()[0])\n",
    "            \n",
    "#         ans = tf.keras.activations.relu(ans)\n",
    "#         ans = ans.numpy()\n",
    "#         # ------------------------------------------------------------------\n",
    "#         model_batch = Sequential()\n",
    "#         model_batch.add(layers.BatchNormalization())\n",
    "#         ans = model_batch.predict(ans)   \n",
    "        # ------------------------------------------------------------------\n",
    "#         model_max = Sequential()\n",
    "#         model_max.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "#         (x, y, z) = ans.shape\n",
    "#         ans = ans.reshape(1, x, y, z)\n",
    "#         ans = model_max.predict(ans)   \n",
    "#         # ------------------------------------------------------------------\n",
    "#         model_drop = Sequential()\n",
    "#         model_drop.add(layers.Dropout(0.3))\n",
    "#         ans = model_drop.predict(ans) \n",
    "#         print('after drop',ans.shape)\n",
    "        # ------------------------------------------------------------------\n",
    "        model_flatten = Sequential()\n",
    "        model_flatten.add(layers.Flatten())\n",
    "        (m, n) = model_flatten.predict(ans).shape\n",
    "        ans = model_flatten.predict(ans).reshape(1, m * n)\n",
    "        print('after flatten',ans.shape)\n",
    "        # ------------------------------------------------------------------\n",
    "#         if sim == 'analog':\n",
    "#             ans = fc(ans, model.layers[5].get_weights()[0])\n",
    "#         elif sim == 'digital':\n",
    "#             ans = fc(ans, model.layers[5].get_weights()[0])        \n",
    "#         # ------------------------------------------------------------------\n",
    "#         model_batch = Sequential()\n",
    "#         model_batch.add(layers.BatchNormalization())\n",
    "#         ans = model_batch.predict(ans)           \n",
    "#         # ------------------------------------------------------------------\n",
    "#         model_drop = Sequential()\n",
    "#         model_drop.add(layers.Dropout(0.5))\n",
    "#         ans = model_drop.predict(ans)            \n",
    "        # ------------------------------------------------------------------                          \n",
    "        print(inputs.shape)\n",
    "        if sim == 'analog':\n",
    "            ans = fc(ans, model.layers[3].get_weights()[0])\n",
    "        elif sim == 'digital':\n",
    "            ans = fc(ans, model.layers[3].get_weights()[0])\n",
    "        # ------------------------------------------------------------------  \n",
    "        if sim == 'analog':\n",
    "            pred_analog.append(np.argmax(ans))  \n",
    "        elif sim == 'digital':\n",
    "            pred_digital.append(np.argmax(ans))  \n",
    "            \n",
    "            \n",
    "for i in range(len(pred_digital)):\n",
    "    print(pred_digital[i], pred_analog[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_analog)):\n",
    "    print(pred_analog[i], pred_digital[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in model.predict(test_images):\n",
    "    print(np.argmax(i), end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.layers[7].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[6].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_75%.ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans4[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ans5 = model2.predict(ans4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(inputO_, kernelO_):\n",
    "    input_ = deepcopy(inputO_)\n",
    "    kernel_ = deepcopy(kernelO_)\n",
    "    \n",
    "    (ix, iy, iz) = input_.shape\n",
    "    (kx, ky, kz, kc) = kernel_.shape\n",
    "    \n",
    "#     iOut = np.zeros(ix* iy* iz)\n",
    "#     iOut = iOut.reshape(ix, iy, iz)\n",
    "    \n",
    "#     iKernel = np.zeros(kx* ky* kz* kc)\n",
    "#     iKernel = iKernel.reshape(kx, ky, kz, kc)\n",
    "    \n",
    "    # ********************* quantizing the input image and kernels ***************************\n",
    "    max_ = np.amax(abs(input_))\n",
    "    for ix_ in range(ix):\n",
    "        for iy_ in range(iy):\n",
    "            for iz_ in range(iz):\n",
    "                input_[ix_][iy_][iz_] = int(np.floor((abs(input_[ix_][iy_][iz_] / max_)) * (2 ** res - 1) )) * np.sign(input_[ix_][iy_][iz_])\n",
    "\n",
    "                    \n",
    "    max_ = np.amax(abs(kernel_))\n",
    "    for kx_ in range(kx):\n",
    "        for ky_ in range(ky):\n",
    "            for kz_ in range(kz):\n",
    "                for kc_ in range(kc):\n",
    "                    kernel_[kx_][ky_][kz_][kc_] = int(np.floor((abs(kernel_[kx_][ky_][kz_][kc_] / max_)) * (2 ** res -1 )))  * np.sign(kernel_[kx_][ky_][kz_][kc_])\n",
    "\n",
    "\n",
    "    #print(kernel_)\n",
    "    #print(input_)\n",
    "    ans = np.zeros(kc * (iy - ky +1) * (iy - ky +1))\n",
    "    ans = ans.reshape((iy - ky +1), (iy - ky +1), kc)\n",
    "    for kc_ in range(kc):\n",
    "        for kz_ in range(kz):\n",
    "            # 32 - 3 + 1 = 30 - 29 + 2 = 31\n",
    "            for i in range((iy - ky + 1)):\n",
    "                for j in range((iy - ky + 1)):\n",
    "                        for ky_ in range(ky):\n",
    "                            for kx_ in range(kx):\n",
    "                                    ans[i][j][kc_] += input_[kx_ + i][ky_ + j][kz_] * kernel_[kx_][ky_][kz_][kc_]\n",
    "                                    \n",
    "                                    \n",
    "    max_ = np.amax(abs(ans))            \n",
    "    for kc_ in range(kc):\n",
    "        for i in range((iy - ky + 1)):\n",
    "            for j in range((iy - ky + 1)):\n",
    "                ans[i][j][kc_] = int(np.floor((abs(ans[i][j][kc_] / max_)) * (2 ** res - 1) )) * np.sign(ans[i][j][kc_])\n",
    "    return ans\n",
    "\n",
    "res = 4\n",
    "\n",
    "import copy \n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def conv2dAnalog(inputO_, kernelO_):\n",
    "    input_ = deepcopy(inputO_)\n",
    "    kernel_ = deepcopy(kernelO_)\n",
    "    \n",
    "    (ix, iy, iz) = input_.shape\n",
    "    (kx, ky, kz, kc) = kernel_.shape\n",
    "    \n",
    "    iOut = np.zeros(ix* iy* iz)\n",
    "    iOut = iOut.reshape(ix, iy, iz)\n",
    "    \n",
    "    iKernel = np.zeros(kx* ky* kz* kc)\n",
    "    iKernel = iKernel.reshape(kx, ky, kz, kc)\n",
    "    \n",
    "    # ********************* quantizing the input image and kernels ***************************\n",
    "    max_ = np.amax(abs(input_))\n",
    "    for ix_ in range(ix):\n",
    "        for iy_ in range(iy):\n",
    "            for iz_ in range(iz):\n",
    "                input_[ix_][iy_][iz_] = int(np.floor((abs(input_[ix_][iy_][iz_] / max_)) * (2 ** res - 1) )) * np.sign(input_[ix_][iy_][iz_])\n",
    "                if input_[ix_][iy_][iz_] == 0:\n",
    "                    iOut[ix_][iy_][iz_] = 0.9\n",
    "                else:\n",
    "                    iOut[ix_][iy_][iz_]  = (abs(input_[ix_][iy_][iz_]) * 0.04 + 0.9) * np.sign(input_[ix_][iy_][iz_])  \n",
    "    #print(iOut)\n",
    "    max_ = np.amax(abs(kernel_))\n",
    "    for kx_ in range(kx):\n",
    "        for ky_ in range(ky):\n",
    "            for kz_ in range(kz):\n",
    "                for kc_ in range(kc):\n",
    "                    kernel_[kx_][ky_][kz_][kc_] = int(np.floor((abs(kernel_[kx_][ky_][kz_][kc_] / max_)) * (2 ** res -1 )))  * np.sign(kernel_[kx_][ky_][kz_][kc_])\n",
    "                    if kernel_[kx_][ky_][kz_][kc_] == 0:\n",
    "                        iKernel[kx_][ky_][kz_][kc_] = 1.08\n",
    "                    else:\n",
    "                        iKernel[kx_][ky_][kz_][kc_]  = (abs(kernel_[kx_][ky_][kz_][kc_])  * 0.02 + 1.08) * np.sign(kernel_[kx_][ky_][kz_][kc_])                                  \n",
    "   # ***************************    convert to analog values  ********************************\n",
    "\n",
    "\n",
    "    ansP = np.zeros(kc * (iy - ky +1) * (iy - ky +1))\n",
    "    ansP = ansP.reshape((iy - ky +1), (iy - ky +1), kc)\n",
    "\n",
    "    ansN = np.zeros(kc * (iy - ky +1) * (iy - ky +1))\n",
    "    ansN = ansN.reshape((iy - ky +1), (iy - ky +1), kc)\n",
    "    \n",
    "    ans = np.zeros(kc * (iy - ky +1) * (iy - ky +1))\n",
    "    ans = ans.reshape((iy - ky +1), (iy - ky +1), kc)\n",
    "    \n",
    "    deb = []\n",
    "    for kc_ in range(kc):\n",
    "        print(kc_, end = ' ')\n",
    "        for kz_ in range(kz):\n",
    "            # 32 - 3 + 1 = 30 - 29 + 2 = 31\n",
    "            for i in range((iy - ky + 1)):\n",
    "                for j in range((iy - ky + 1)):\n",
    "                        for ky_ in range(ky):\n",
    "                            for kx_ in range(kx):\n",
    "                                if i == 0 and j == 0 :\n",
    "                                    deb.append((iOut[kx_ + i][ky_ + j][kz_], iKernel[kx_][ky_][kz_][kc_]))\n",
    "                                if iOut[kx_ + i][ky_ + j][kz_] * iKernel[kx_][ky_][kz_][kc_] > 0:\n",
    "                                    ansP[i][j][kc_] += 1 / f.lookUp(1.8, abs(iKernel[kx_][ky_][kz_][kc_]), abs(iOut[kx_ + i][ky_ + j][kz_]))\n",
    "                                elif iOut[kx_ + i][ky_ + j][kz_] * iKernel[kx_][ky_][kz_][kc_] < 0:\n",
    "                                    ansN[i][j][kc_] += 1 / f.lookUp(1.8, abs(iKernel[kx_][ky_][kz_][kc_]), abs(iOut[kx_ + i][ky_ + j][kz_] ))\n",
    "                                else:\n",
    "                                    ansP[i][j][kc_] += 1 / f.lookUp(1.8, 1.08, 0.9)\n",
    "                                    ansN[i][j][kc_] += 1 / f.lookUp(1.8, 1.08, 0.9)\n",
    "    cap = 10e-15\n",
    "    for kc_ in range(kc):\n",
    "        for i in range((iy - ky + 1)):\n",
    "            for j in range((iy - ky + 1)):\n",
    "                ansP[i][j][kc_] = 1.8 * math.exp((-1e-9) / ((1 / ansP[i][j][kc_]) * cap))\n",
    "                ansN[i][j][kc_] = 1.8 * math.exp((-1e-9) / ((1 / ansN[i][j][kc_]) * cap))\n",
    "                \n",
    "                ans[i][j][kc_] = ansN[i][j][kc_] - ansP[i][j][kc_]\n",
    "                \n",
    "    max_ = np.amax(abs(ans))            \n",
    "    for kc_ in range(kc):\n",
    "        for i in range((iy - ky + 1)):\n",
    "            for j in range((iy - ky + 1)):\n",
    "                ans[i][j][kc_] = int(np.floor((abs(ans[i][j][kc_] / max_)) * (2 ** res - 1) )) * np.sign(ans[i][j][kc_])\n",
    "                \n",
    "    return ans\n",
    "\n",
    "\n",
    "def fcAnalog(inputO_, kernelO_):\n",
    "    \n",
    "    input_ = deepcopy(inputO_)\n",
    "    kernel_ = deepcopy(kernelO_)\n",
    "    \n",
    "    (ix, iy) = input_.shape\n",
    "    (kx, ky) = kernel_.shape\n",
    "    \n",
    "    print(kx, ky)\n",
    "    \n",
    "    iOut = np.zeros(kx)\n",
    "    iOut = iOut.reshape(1, kx)\n",
    "    \n",
    "    iKernel = np.zeros(kx *  ky)\n",
    "    iKernel = iKernel.reshape(kx, ky)\n",
    "    \n",
    "    # ********************* quantizing the input image and kernels ***************************\n",
    "    max_ = np.amax(abs(input_))\n",
    "    for kx_ in range(kx):\n",
    "        input_[0][kx_] = int(np.floor((abs(input_[0][kx_] / max_)) * (2 ** res - 1) )) * np.sign(input_[0][kx_])\n",
    "        if input_[0][kx_] == 0:\n",
    "            iOut[0][kx_] = 0.9\n",
    "        else:\n",
    "            iOut[0][kx_] = (abs(input_[0][kx_]) * 0.04 + 0.9) * np.sign(input_[0][kx_])  \n",
    "    #print(iOut)\n",
    "    max_ = np.amax(abs(kernel_))\n",
    "    for kx_ in range(kx):\n",
    "        for ky_ in range(ky):\n",
    "            kernel_[kx_][ky_] = int(np.floor((abs(kernel_[kx_][ky_] / max_)) * (2 ** res -1 )))  * np.sign(kernel_[kx_][ky_])\n",
    "            if kernel_[kx_][ky_] == 0:\n",
    "                iKernel[kx_][ky_] = 1.08\n",
    "            else:\n",
    "                iKernel[kx_][ky_]  = (abs(kernel_[kx_][ky_])  * 0.02 + 1.08) * np.sign(kernel_[kx_][ky_])                                  \n",
    "   # ***************************    convert to analog values  ********************************\n",
    "\n",
    "#     print(iKernel)\n",
    "#     print(iOut)\n",
    "    ansP = np.zeros(ky)\n",
    "    ansP = ansP.reshape(1, ky)\n",
    "\n",
    "    ansN = np.zeros(ky)\n",
    "    ansN = ansN.reshape(1, ky)\n",
    "    \n",
    "    ans = np.zeros(ky)\n",
    "    ans = ans.reshape(1, ky)\n",
    "    \n",
    "\n",
    "    for ky_ in range(ky):\n",
    "        for kx_ in range(kx):\n",
    "            if iOut[0][kx_] * iKernel[kx_][ky_] > 0:\n",
    "                ansP[0][ky_] += 1 / f.lookUp(1.8, abs(iKernel[kx_][ky_]), abs(iOut[0][kx_]))\n",
    "            elif iOut[0][kx_] * iKernel[kx_][ky_] < 0:\n",
    "                ansN[0][ky_] += 1 / f.lookUp(1.8, abs(iKernel[kx_][ky_]), abs(iOut[0][kx_] ))\n",
    "            else:\n",
    "                ansP[0][ky_] += 1 / f.lookUp(1.8, 1.08, 0.9)\n",
    "                ansN[0][ky_] += 1 / f.lookUp(1.8, 1.08, 0.9)\n",
    "    cap = 10e-15\n",
    "    for ky_ in range(ky):\n",
    "        ansP[0][ky_] = 1.8 * math.exp((-1e-9) / ((1 / ansP[0][ky_]) * cap))\n",
    "        ansN[0][ky_] = 1.8 * math.exp((-1e-9) / ((1 / ansN[0][ky_]) * cap))\n",
    "        ans[0][ky_] = ansN[0][ky_] - ansP[0][ky_]\n",
    "#     print(ansN)\n",
    "#     print(ans)\n",
    "    \n",
    "    max_ = np.amax(abs(ans))            \n",
    "    for ky_ in range(ky):\n",
    "            ans[0][ky_] = int(np.floor((abs(ans[0][ky_] / max_)) * (2 ** res - 1) )) * np.sign(ans[0][ky_])\n",
    "    \n",
    "    return ans\n",
    "\n",
    "# ansAnalog = conv2dAnalog(testX[2], model.layers[0].get_weights()[0])\n",
    "# ansIdeal = conv2d(testX[2], model.layers[0].get_weights()[0])\n",
    "def fc(inputO_, kernelO_):\n",
    "    \n",
    "    input_ = deepcopy(inputO_)\n",
    "    kernel_ = deepcopy(kernelO_)\n",
    "    \n",
    "    (ix, iy) = input_.shape\n",
    "    (kx, ky) = kernel_.shape\n",
    "    \n",
    "    print(kx, ky)\n",
    "    \n",
    "    iOut = np.zeros(kx)\n",
    "    iOut = iOut.reshape(1, kx)\n",
    "    \n",
    "    iKernel = np.zeros(kx *  ky)\n",
    "    iKernel = iKernel.reshape(kx, ky)\n",
    "    \n",
    "    # ********************* quantizing the input image and kernels ***************************\n",
    "    max_ = np.amax(abs(input_))\n",
    "    for kx_ in range(kx):\n",
    "        input_[0][kx_] = int(np.floor((abs(input_[0][kx_] / max_)) * (2 ** res - 1) )) * np.sign(input_[0][kx_])\n",
    "\n",
    "    #print(iOut)\n",
    "    max_ = np.amax(abs(kernel_))\n",
    "    for kx_ in range(kx):\n",
    "        for ky_ in range(ky):\n",
    "            kernel_[kx_][ky_] = int(np.floor((abs(kernel_[kx_][ky_] / max_)) * (2 ** res -1 )))  * np.sign(kernel_[kx_][ky_])\n",
    "    ans = np.dot(input_,kernel_)\n",
    "    \n",
    "    max_ = np.amax(abs(ans))            \n",
    "    for ky_ in range(ky):\n",
    "            ans[0][ky_] = int(np.floor((abs(ans[0][ky_] / max_)) * (2 ** res - 1) )) * np.sign(ans[0][ky_])    \n",
    "    return ans\n",
    "                                                                                                           \n",
    "# fcAnalog(fcBefore, model.layers[2].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RegularGridInterpolator\n",
    "class calc:\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv('data_vds_included.csv')\n",
    "        self.featureID = None\n",
    "        self.vb = np.arange (0.9, 1.51, 0.04)\n",
    "        self.vg = np.arange (1.08, 1.381, 0.02)\n",
    "        self.vd   = np.arange (1.1, 1.81, 0.1) \n",
    "        self.interp = None\n",
    "        self.numObs = None \n",
    "        self.dict_signals = None\n",
    "        self.xw_sets = None \n",
    "        self.numClass = 45\n",
    "        self.Rn_total = None\n",
    "        self.Rp_total = None\n",
    "        self.bc_labels = [\"0_1\", \"0_2\", \"0_3\", \"0_4\", \"0_5\", \"0_6\", \"0_7\", \"0_8\", \"0_9\", \"1_2\", \"1_3\", \"1_4\", \"1_5\", \"1_6\", \"1_7\", \"1_8\", \"1_9\", \"2_3\", \"2_4\", \"2_5\", \"2_6\", \"2_7\", \"2_8\", \"2_9\", \"3_4\", \"3_5\", \"3_6\", \"3_7\", \"3_8\", \"3_9\", \"4_5\", \"4_6\", \"4_7\", \"4_8\", \"4_9\", \"5_6\", \"5_7\", \"5_8\", \"5_9\", \"6_7\", \"6_8\", \"6_9\", \"7_8\", \"7_9\", \"8_9\"]\n",
    "        self.bc_dict = {\"0_1\":0, \"0_2\":1, \"0_3\":2, \"0_4\":3, \"0_5\":4, \"0_6\":5, \"0_7\":6, \"0_8\":7, \"0_9\":8, \"1_2\":9, \"1_3\":10, \"1_4\":11, \"1_5\":12, \"1_6\":13, \"1_7\":14, \"1_8\":15, \"1_9\":16, \"2_3\":17, \"2_4\":18, \"2_5\":19, \"2_6\":20, \"2_7\":21, \"2_8\":22, \"2_9\":23, \"3_4\":24, \"3_5\":25, \"3_6\":26, \"3_7\":27, \"3_8\":28, \"3_9\":29, \"4_5\":30, \"4_6\":31, \"4_7\":32, \"4_8\":33, \"4_9\":34, \"5_6\":35, \"5_7\":36, \"5_8\":37, \"5_9\":38, \"6_7\":39, \"6_8\":40, \"6_9\":41, \"7_8\":42, \"7_9\":43, \"8_9\":44}\n",
    "    \n",
    "    def preProc(self):\n",
    "        self.data = self.data.iloc[::-1]\n",
    "        self.data = np.array(self.data.iloc[:8,1:]).reshape(8, 16, 16) # (vd, vg, vb)\n",
    "        return \n",
    "        \n",
    "    def interP(self):\n",
    "        self.interp = RegularGridInterpolator((self.vd,self.vg,self.vb), self.data)\n",
    "        return \n",
    "\n",
    "    def lookUp(self, vd, vg, vb):\n",
    "        return abs(self.interp(([[vd, vg, vb]])))\n",
    "    \n",
    "    def calcRes(self, numObs, vd):\n",
    "        self.numObs = numObs\n",
    "        self.Rn_total = []\n",
    "        self.Rp_total = []\n",
    "        for classifier in range(self.numClass):\n",
    "            Rn_classifer = []\n",
    "            Rp_classifer = []\n",
    "            \n",
    "            for obs in range(self.numObs):\n",
    "                Rn_single = []\n",
    "                Rp_single = []\n",
    "                for feature in featureID[classifier]:\n",
    "\n",
    "                    if (xte_cad.iloc[obs, feature]) != 0:\n",
    "                        x = xte_cad.iloc[obs, feature] / 1000\n",
    "                    else: \n",
    "                        x = 0.9\n",
    "                        \n",
    "                    if expBinW.iloc[classifier, feature] != 0:\n",
    "                        w = (abs(expBinW.iloc[classifier, feature]) * 20 + 1080) / 1000\n",
    "                    else: \n",
    "                        w = 1.08\n",
    "                        \n",
    "                    if expBinW.iloc[classifier, feature] >= 0:\n",
    "                            Rp_single.append(1 / f.lookUp(vd, w, x))\n",
    "                            Rn_single.append(1 / f.lookUp(vd, 1.08, 0.9))\n",
    "                    elif expBinW.iloc[classifier, feature] <= 0:\n",
    "                            Rn_single.append(1 / f.lookUp(vd, w, x))\n",
    "                            Rp_single.append(1 / f.lookUp(vd, 1.08, 0.9))\n",
    "                    \n",
    "\n",
    "                Rn_classifer.append(1 / (sum(Rn_single)))\n",
    "                Rp_classifer.append(1 / (sum(Rp_single)))\n",
    "            \n",
    "            self.Rn_total.append(Rn_classifer)\n",
    "            self.Rp_total.append(Rp_classifer)\n",
    "        \n",
    "        return self.Rn_total, self.Rp_total     \n",
    "    def calcTav(self, Rn_total, Rp_total, cap):\n",
    "        dict_tavN = {}\n",
    "        dict_tavP = {}\n",
    "        c = 0\n",
    "        for i in range(10):\n",
    "            for j in range(i + 1, 10):        \n",
    "                for obs in range(self.numObs):\n",
    "                    if 'LN%d_%d'%(i, j) not in dict_tavN:\n",
    "                        dict_tavN['LN%d_%d'%(i, j)] = []\n",
    "\n",
    "\n",
    "                    if 'LP%d_%d'%(i, j) not in dict_tavP:\n",
    "                        dict_tavP['LP%d_%d'%(i, j)] = []\n",
    "\n",
    "                    dict_tavN['LN%d_%d'%(i, j)].append(float(self.Rn_total[c][obs] * cap))\n",
    "                    dict_tavP['LP%d_%d'%(i, j)].append(float(self.Rp_total[c][obs] * cap))\n",
    "                c += 1\n",
    "        return dict_tavN, dict_tavP\n",
    "    def debugCode(self, obs):\n",
    "        self.xw_sets = {}\n",
    "        for classifier in range(self.numClass):\n",
    "            if self.bc_labels[classifier] not in self.xw_sets:\n",
    "                self.xw_sets['LP' + self.bc_labels[classifier]] = []\n",
    "                self.xw_sets['LN' + self.bc_labels[classifier]] = []\n",
    "\n",
    "            for feature in featureID[classifier]:\n",
    "                if expBinW.iloc[classifier, feature] > 0:\n",
    "                    self.xw_sets['LP' + self.bc_labels[classifier]].append((((expBinW.iloc[classifier, feature] * 20) + 1080) / 1000, (xte_cad.iloc[obs, feature]) / 1000))\n",
    "                elif expBinW.iloc[classifier, feature] < 0:\n",
    "                    self.xw_sets['LN' + self.bc_labels[classifier]].append(((abs(expBinW.iloc[classifier, feature] * 20) + 1080) / 1000, (xte_cad.iloc[obs, feature]) / 1000))\n",
    "        return self.xw_sets\n",
    "\n",
    "    def dataCadence(self):\n",
    "        data = PSF(simDir + outDir + \"/nom/Dotran.tran.tran\")\n",
    "        signal_names = []\n",
    "        for i in range(10):\n",
    "            for j in range(i + 1, 10):\n",
    "                signal_names.append(\"LN%d_%d\"%(i, j))\n",
    "        for i in range(10):\n",
    "            for j in range(i + 1, 10):        \n",
    "                signal_names.append(\"LP%d_%d\"%(i, j))\n",
    "        self.dict_signals = {}\n",
    "        for signal_name in signal_names:\n",
    "            for signal in data.all_signals():\n",
    "                if signal.name == signal_name:\n",
    "                    self.dict_signals[signal.name] = signal.ordinate\n",
    "        return self.dict_signals        \n",
    "    def debugRes(self, bc, neg_or_pos, obs):\n",
    "        #print((bc, neg_or_pos, obs))\n",
    "        if neg_or_pos == 'P':\n",
    "            print(self.Rp_total[self.bc_dict[\"%d_%d\"%(bc[0], bc[1])]][obs])\n",
    "        elif neg_or_pos == 'N':\n",
    "            print(self.Rn_total[self.bc_dict[\"%d_%d\"%(bc[0], bc[1])]][obs])\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "f = calc()\n",
    "f.preProc()\n",
    "f.interP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
